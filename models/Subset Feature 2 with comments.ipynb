{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Trying out Subset Feature, second try with a bit of chat gpt help",
   "id": "77037e7f285c370"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader"
   ],
   "id": "edb4cb139a747ac9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The data_transforms dictionary contains two keys: 'train' and 'val', each associated with a transforms.Compose object.  This object is a sequential container that applies a list of transformations to the data",
   "id": "81aa1dc7bf0256c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224), #This transformation crops a random portion of the image and resizes it to 224x224 pixels.\n",
    "        transforms.RandomHorizontalFlip(), #This randomly flips the image horizontally with a probability of 0.5.\n",
    "        transforms.ToTensor(), #Converts the image (a PIL Image or numpy.ndarray) into a PyTorch tensor.\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) #Normalizes the tensor image with selected mean and SD\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ],
   "id": "79e14b35076da9d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This section of the code below is responsible for loading the data, applying the transformations,\n",
    "and setting up the data loaders for training and validation\n",
    "\n",
    "1. data_dir\n",
    "\n",
    "- Purpose: This line specifies the root directory where the dataset is stored.\n",
    "- Usage: data_dir will be used to construct the paths for the training and validation datasets\n",
    "\n",
    "2. image_dataset\n",
    "\n",
    "- Purpose: This dictionary comprehension creates a dataset for both the training and validation sets using the ImageFolder class from torchvision.datasets.\n",
    "- Details:\n",
    "- os.path.join(data_dir, x):\n",
    "Constructs the path to the dataset directories, e.g., path_to_data/train and path_to_data/val.\n",
    "data_transforms[x]:\n",
    "Applies the appropriate transformations (train or val) to the dataset.\n",
    "\n",
    "3.dataloaders\n",
    "- Purpose: This dictionary comprehension creates data loaders for both the training and validation datasets.\n",
    "- Details:\n",
    "- DataLoader:\n",
    "PyTorch's DataLoader class combines a dataset and a sampler, and provides an iterable over the given dataset.\n",
    "Parameters:\n",
    "- image_datasets[x]: The dataset to load (either training or validation).\n",
    "- batch_size=32: Number of samples per batch to load.\n",
    "- shuffle=True: Shuffles the data at every epoch (important for training to ensure randomness).\n",
    "- num_workers=4: Number of subprocesses to use for data loading. More workers can speed up data loading but requires more CPU resources.\n",
    "\n",
    "4. dataset_sizes\n",
    "- Purpose: This dictionary comprehension computes the size (number of images) of both the training and validation datasets.\n",
    "- Usage: Knowing the dataset sizes is useful for tracking training progress and for calculating metrics like accuracy.\n",
    "\n",
    "5. class_names\n",
    "- Purpose: Extracts the class names from the training dataset.\n",
    "- Details:\n",
    "image_datasets['train'].classes:\n",
    "- The ImageFolder class automatically assigns a list of class names based on the sub-directory names. For instance, if the training dataset has sub-directories named 'dog' and 'cat', classes will be ['cat', 'dog'].\n",
    "- Usage: Knowing the class names is helpful for interpreting the model's predictions and for any visualization purposes.\n"
   ],
   "id": "5da0f2ed59026f7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data_dir = 'path_to_data' #TODO change root directory to data set\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']} #TODO change the image folder to the one we want?\n",
    "dataloaders = {x: DataLoader(image_datasets[x], batch_size=32, shuffle=True, num_workers=4) for x in ['train', 'val']} #TODO CAN THIS BE CHANGED TO GPU or azure?\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n"
   ],
   "id": "fd44ced840c47f10"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Learning Model \n",
    "\n"
   ],
   "id": "db0a893f5ee5a67c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "### Learning Model\n",
    "#The SubsetFeatureLearningModel class is a neural network model designed for fine-grained category classification,\n",
    "# leveraging the principles of subset feature learning. This model uses a pre-trained ResNet50 backbone and adds both a\n",
    "# main classifier and several subset classifiers.\n",
    "#TODO do we want to change to another pretrained model?\n",
    "\n",
    "class SubsetFeatureLearningModel(nn.Module):\n",
    "    #Inheritance from nn.Module: The model inherits from torch.nn.Module, which is the base class for all neural network modules in PyTorch.\n",
    "    def __init__(self, num_classes, subset_classes):\n",
    "        super(SubsetFeatureLearningModel, self).__init__()\n",
    "        self.backbone = models.resnet50(pretrained=True)\n",
    "        num_ftrs = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()  # Removing the original classifier\n",
    "\n",
    "        # Main classifier\n",
    "        self.fc_main = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "        # Subset classifiers\n",
    "        self.fc_subsets = nn.ModuleList([nn.Linear(num_ftrs, sub_classes) for sub_classes in subset_classes])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        main_out = self.fc_main(x)\n",
    "        subset_outs = [fc(x) for fc in self.fc_subsets]\n",
    "        return main_out, subset_outs\n",
    "\n",
    "\n",
    "# Define the number of classes and subsets\n",
    "num_classes = len(class_names)\n",
    "subset_classes = [50, 50, 50, 50]  # Example subsets\n",
    "\n",
    "model = SubsetFeatureLearningModel(num_classes, subset_classes)\n",
    "\n",
    "def train_model(model, criterion, optimizer, num_epochs=25):\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    main_out, subset_outs = model(inputs)\n",
    "                    _, preds = torch.max(main_out, 1)\n",
    "                    loss_main = criterion(main_out, labels)\n",
    "                    loss_subset = sum(criterion(sub_out, labels) for sub_out in subset_outs)\n",
    "                    loss = loss_main + loss_subset\n",
    "\n",
    "                    # Backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "    return model\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "model = train_model(model, criterion, optimizer, num_epochs=25)\n",
    "\n",
    "def evaluate_model(model):\n",
    "    model.eval()\n",
    "    running_corrects = 0\n",
    "\n",
    "    for inputs, labels in dataloaders['val']:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            main_out, _ = model(inputs)\n",
    "            _, preds = torch.max(main_out, 1)\n",
    "\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    acc = running_corrects.double() / dataset_sizes['val']\n",
    "    print(f'Validation Accuracy: {acc:.4f}')\n",
    "\n",
    "evaluate_model(model)"
   ],
   "id": "e3b20016421a203c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
